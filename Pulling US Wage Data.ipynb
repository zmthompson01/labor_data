{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling and Cleaning US Wage Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages, Libraries, and Working Directory\n",
    "Below are the tools used in the analysis. If a user is looking to run everything themselves, change the file path below. `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob ### group together files\n",
    "import os ### manage file path\n",
    "import pandas as pd ### dataframe management and manipulation\n",
    "import numpy as np ### numeric functions such as averages and sums\n",
    "from urllib import urlretrieve ### accessing BLS datasets from the website\n",
    "from time import time ### clock to time sections of the code\n",
    "from datetime import datetime ### getting years other date parts\n",
    "import zipfile ### unzipping the BLS datasets\n",
    "from shutil import rmtree ### clearing folders after being merged\n",
    "\n",
    "### plots show in Jupyter Notebook\n",
    "%matplotlib inline \n",
    "\n",
    "### Adjust the below path per use case\n",
    "path = \"YOUR/FILE/PATH/HERE\"\n",
    "os.chdir(path)\n",
    "\n",
    "### Check if there is an existing data folder to house downloads\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\")\n",
    "    print(\"Made folder for downloads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Mining and Cleaning\n",
    "Data from this project has been pulled from the BLS site. The first file pulled is a comprehensive list of MSAs, their counties, and the codes the unique codes to identify them. Wage and wage statistics are downloaded next. Each zip folder has about 4 Gb of data and 4,200 CSV files and there is a zip file for each year. Processing the data is out of the scope of this project, so we will focus on data from 2005 onward and only in the construction industry.\n",
    "\n",
    "Data has already been pre-loaded and cleaned for the purposes of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download MSA area definitions\n",
    "The area_definitions file defines which counties are in every MSA.  Additionally, the below removes US terretories (except Puerto Rico) and US military junctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area definitions are already there and cleaned\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "### check for file in data folder\n",
    "if not os.path.exists(\"data/area_definitions_m2016.xlsx\"): ### check id file exists\n",
    "    print(\"Downloading area definitions\")\n",
    "    url_msaDef = \"https://www.bls.gov/oes/current/area_definitions_m2016.xlsx\" ### file URL\n",
    "    area_def = urlretrieve(url_msaDef, \"data/area_definitions_m2016.xlsx\") ### download the file into data repository\n",
    "    print(\"Downloaded in %s seconds\" % round(time()-t0, 1))\n",
    "    \n",
    "    ### Cleaning the MSA definitions file\n",
    "    df_fips = pd.read_excel(\"data/area_definitions_m2016.xlsx\",\n",
    "                        converters={\n",
    "                            \"FIPS code\": str,\n",
    "                            \"MSA code (including MSA divisions)\": str,\n",
    "                            \"County name (or Township name for the New England states)\":str, \n",
    "                            \"County code\": str,\n",
    "                            \"Township code\": str,\n",
    "                            \"MSA code for MSAs with divisions\": str,\n",
    "                            \"MSA name for MSAs with divisions\": str\n",
    "                        })\n",
    "    os.remove(\"data/area_definitions_m2016.xlsx\")\n",
    "    df_fips.rename(columns={\"MSA code (including MSA divisions)\":\"MSA_code\"}, inplace=True) ### rename for ease of use\n",
    "    \n",
    "    ### Each county is broken into two separate codes, the first is the FIPS code which is a 2-digit identifier for\n",
    "    ### the state, region, or terretory it is in. The second is the county code, a 3-digit identifer that is unique\n",
    "    ### to each county in a state. The combiniation of the two codes makes a unique ID for each county at a national\n",
    "    ### view that's ideal to work with in this analysis\n",
    "    df_fips[\"area_fips\"] = df_fips[\"FIPS code\"].str.cat(df_fips[\"County code\"].values.astype(str), sep='')\n",
    "    not_states = [\"60\", \"66\", \"69\", \"74\", \"78\"] #US territories and military bases less Puerto Rico\n",
    "    df_fips = (df_fips[~df_fips[\"FIPS code\"].isin(not_states)])\n",
    "    \n",
    "    ### list of other statistical indicators not pertainant to this research\n",
    "    nonoWords = [\"nonmetropolitan\", \"NECTA\"]\n",
    "    df_fips = (df_fips[~df_fips[\"MSA name (including MSA divisions)\"].str.contains('|'.join(nonoWords))])\n",
    "    \n",
    "    ### Columns already used or not needed for this project\n",
    "    df_fips = df_fips.drop(['FIPS code', 'State', 'State abbreviation', 'County code', \n",
    "                            'Township code','MSA code for MSAs with divisions',\n",
    "                            'MSA name for MSAs with divisions'], axis=1)\n",
    "    df_fips.rename(columns={\"MSA name (including MSA divisions)\":\"MSA_name\",\n",
    "                            \"County name (or Township name for the New England states)\":\"County_name\"},inplace=True)\n",
    "    \n",
    "    ### The CSV file from BLS has spaces before each code that needs to be removed\n",
    "    df_fips['MSA_code'] = df_fips['MSA_code'].str.replace('  ','')\n",
    "    df_fips['area_fips'] = df_fips['area_fips'].str.replace('  ','')\n",
    "    \n",
    "    ### export the results to an Excel file\n",
    "    df_fips.to_excel(\"data/area_definitions_m2016.xlsx\", sheet_name=\"1\", index=False)\n",
    "    \n",
    "else:\n",
    "    print(\"Area definitions are already there and cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download QCEW zip files\n",
    "[QCEW](https://www.bls.gov/cew/) is BLS' repository of quarterly wage data that covers 95% of county, MSA and state level wage statistics. The below code pulls the zip file for each year queried (in this project is the prior 12), opens the zip and extracts all the CSVs. Each CSV is the quarterly statistics for each county in the US. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "year = datetime.today().year ### Get this moment's year\n",
    "print(year)\n",
    "\n",
    "### Go 12 years back from last year since full year current year will not be published\n",
    "years = map(str, range(year, year))\n",
    "print(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in years:\n",
    "    if sum(t.startswith(i + \"_summary\") for t in os.listdir(\"data/\")) == 1: ### check for year's summary file\n",
    "        size_sum = float(os.path.getsize(\"data/\" + i + \"_summary.xlsx\")) ### size of file in bytes\n",
    "        print(i + \" was already processed and is about %.2f MBs\" % (size_sum/1000000)) ### size of file in MBs\n",
    "    elif sum(t.startswith(i + \".q1-q4\") for t in os.listdir(\"data/\")) == 1: ### check for year's raw data\n",
    "        length = len(os.listdir(\"data/\" + i + \".q1-q4.by_area/\"))\n",
    "        size = float(0)\n",
    "        for  l in os.listdir(\"data/\" + i + \".q1-q4.by_area/\"):\n",
    "            size += os.path.getsize(\"data/\" + i + \".q1-q4.by_area/\" + l)\n",
    "        print(i + \" was already downloaded, has %d files, and is about %.2f GBs\" % (length,size/1000000000))    \n",
    "    else: ### download raw data folder for each year\n",
    "        t1 = time()\n",
    "        \n",
    "        for w in os.listdir(\"data/\"):\n",
    "            if w.startswith(i) == True:\n",
    "                os.remove(\"data/\" + w) \n",
    "            \n",
    "        print(\"Downloading \" + i + \" quarterly data\")\n",
    "        ### dynamic URL for each year\n",
    "        url = \"https://www.bls.gov/cew/data/files/\" + i + \"/csv/\" + i + \"_qtrly_by_area.zip\"\n",
    "        urlretrieve(url, \"data/\" + i + \"_qtrly_by_area.zip\") ### download contents from the URL\n",
    "\n",
    "        zip_ref = zipfile.ZipFile(\"data/\" + i + \"_qtrly_by_area.zip\", 'r') ### read the contents of the folder\n",
    "        zip_ref.extractall(path + \"/data\") ### extract the zip file data to the data repository for this project\n",
    "        zip_ref.close()\n",
    "\n",
    "        os.remove(\"data/\" + i + \"_qtrly_by_area.zip\") ### remove the zip folder\n",
    "        \n",
    "        length = len(os.listdir(\"data/\" + i + \".q1-q4.by_area/\"))\n",
    "        size = float(0)\n",
    "        for  l in os.listdir(\"data/\" + i + \".q1-q4.by_area/\"):\n",
    "            size += os.path.getsize(\"data/\" + i + \".q1-q4.by_area/\" + l)\n",
    "        print(i + \" downloaded in ~%s seconds, has %d files and is about %.2f GBs\"\n",
    "              % (round(time() - t1, 1), length, size/1000000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing manageable dataframes\n",
    "BLS pulls together all the counties' data into individual csv files and aggregates them into one zip file by qauarter. We'll use the [glob](https://docs.python.org/2/library/glob.html) package to aggregate all these files (approximately 4,200 per year) into one dataframe that will be easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Read in the MSA definitions to filter by\n",
    "df_fips = pd.read_excel(\"data/area_definitions_m2016.xlsx\",\n",
    "                        index_col=False,\n",
    "                        converters={'MSA_code': str,\n",
    "                                   'area_fips': str})\n",
    "\n",
    "### Industries to filter by\n",
    "# industries = [\"1011\", \"1012\", \"1013\", \"1021\", \"1022\", \"1023\", \"1024\", \"1025\", \"1026\", \"1027\"]\n",
    "industries = [\"1012\"] ### Construction industry\n",
    "\n",
    "concatenated_dfs = [] ### initiate an empty list of dataframes for further use\n",
    "for i in years:  \n",
    "    if sum(t.startswith(i + \"_summary\") for t in os.listdir(\"data/\")) == 1: ### check if summary file has been made\n",
    "        print(i + \" dataframe has already been constructed\")\n",
    "        ### if the summary file has already been made, read in that file and append to concatenated_dfs list\n",
    "        concatenated_dfs.append(pd.read_excel(\"data/\" + i + \"_summary.xlsx\",\n",
    "                                              converters={'MSA_code': str,\n",
    "                                                          'area_fips':str,\n",
    "                                                          'year': str,\n",
    "                                                          'industry_code': str,\n",
    "                                                          'qtr': str}))\n",
    "    else:      \n",
    "        print(\"Building a single dataframe for \" + i)\n",
    "\n",
    "        ### Since we already filtered out the counties and regions we don't need, we'll use the county codes\n",
    "        ### from the MSA definition file\n",
    "        fips_list = df_fips[\"area_fips\"]\n",
    "\n",
    "        t2 = time()\n",
    "        \n",
    "        ### join all csv files from raw data zip folder into a list \n",
    "        all_files = glob.glob(os.path.join(\"data/\" + i + \".q1-q?.by_area\", \"*.csv\"))\n",
    "        \n",
    "        ### Read all csv files from list into Pandas DFs for transformation\n",
    "        df_from_each_file = (pd.read_csv(f,\n",
    "                                         index_col=False,\n",
    "                                         converters={'area_fips': str,\n",
    "                                                     'own_code': str,\n",
    "                                                     'industry_code': str},\n",
    "                                         low_memory=False)\n",
    "                             for f in all_files)\n",
    "\n",
    "        ### Melt all the individual data frames into one\n",
    "        c_df = pd.concat(df_from_each_file, ignore_index=True)\n",
    "\n",
    "        before = c_df.shape\n",
    "\n",
    "        ### Drop unused columns or those already in the MSA definitions file\n",
    "        c_df = c_df.drop(['size_code','disclosure_code','area_title','own_title','industry_title',\n",
    "                          'agglvl_title','agglvl_code','size_title','lq_disclosure_code','oty_disclosure_code',]\n",
    "                         ,axis=1)\n",
    "\n",
    "        ### Get data only from private sector. Government jobs are skewed by prevailing wages\n",
    "        c_df = c_df[c_df['own_code'] == \"5\"]\n",
    "        c_df = c_df.drop(['own_code'], axis=1)\n",
    "\n",
    "        ### Reduce data to only counties in MSAs \n",
    "        c_df = c_df[c_df['area_fips'].isin(fips_list)]\n",
    "\n",
    "        ### Only work with industries as defined in the list above (for this project it's just construction)\n",
    "        c_df = c_df[c_df['industry_code'].isin(industries)]\n",
    "\n",
    "        ### Use counties only in the MSA definitions file\n",
    "        c_df = pd.merge(df_fips, c_df, on=\"area_fips\", how='right')\n",
    "        c_df = c_df.drop(['MSA_name', 'County_name'], axis=1)\n",
    "\n",
    "        after = c_df.shape\n",
    "\n",
    "        concatenated_dfs.append(c_df) ### append the df from this loop the the main list\n",
    "\n",
    "        print(i + \" data cleaning time is ~%s seconds, dimensions before cleaning were %s, and %s after\"\n",
    "              % (round(time() - t2, 1), before, after))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
